# Final Project: Using ChatGPT to Understand Difficult Code

## Overview

Understanding others’ code is a fundamental challenge for developers. This project evaluates how ChatGPT can assist in this process, focusing on prompt engineering to optimize its explanations. A hypothetical user experiment is also designed to compare ChatGPT’s effectiveness against other resources.

## Repository Structure

```
Final_Project_GitHub_Repo/
├── code/
│   └── example_code.py       # Example code snippet for testing
├── results/
│   └── chatgpt_responses.md  # ChatGPT responses to various prompts
├── docs/
│   ├── README.md             # Project overview
│   ├── project_report.md     # Detailed project report
│   └── presentation.pptx     # Final presentation slides
```

## Experiment Design

We compared three groups:

1. **Group A**: Used ChatGPT to understand code.
2. **Group B**: Used general internet resources (e.g., Stack Overflow).
3. **Group C**: No external assistance.

The groups analyzed the same complex deep learning code snippet. Results were evaluated based on clarity, accuracy, and depth of understanding.

## How to Run

1. **Explore the Code**:
    - Navigate to the `code/` folder and review `example_code.py`.
2. **View ChatGPT Responses**:
    - Check the `results/chatgpt_responses.md` file for prompt-based responses.
3. **Read the Report**:
    - Open `docs/project_report.md` for detailed findings and conclusions.
4. **View the Presentation**:
    - Open `docs/presentation.pptx` to see the final presentation slides.

## Highlights

-   **Structured Prompts**: Yielded the most effective responses.
-   **Clarity**: ChatGPT responses were clearer and more concise than general internet sources.
-   **Depth**: ChatGPT provided better insights with structured prompts.

## Future Work

-   Conduct real-world user experiments with diverse participants.
-   Expand analysis to larger datasets and more complex scenarios.
